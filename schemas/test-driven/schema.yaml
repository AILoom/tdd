name: test-driven
version: 1
description: Default TDD workflow — intent → test-plan → design → tasks

artifacts:
  - id: intent
    generates: intent.md
    description: Initial intent document outlining why this change is needed
    template: intent.md
    instruction: |
      Create the intent document that establishes WHY this change is needed.

      Sections:
      - **Why**: 1-2 sentences on the problem or opportunity. What problem does this solve? Why now?
      - **What Changes**: Bullet list of behavioral changes. Be specific about new capabilities, modifications, or removals. Mark breaking changes with **BREAKING**.
      - **Test Domains**: Identify which test domains will be created or modified:
        - **New Domains**: List test domains being introduced. Each becomes a new `coverage/<domain>/tests.md`. Use kebab-case names (e.g., `user-auth`, `data-export`).
        - **Modified Domains**: List existing domains whose test coverage is changing. Check `tdd/coverage/` for existing domain names.
      - **Impact**: Affected code, APIs, dependencies, or systems.

      IMPORTANT: The Test Domains section creates the contract between
      intent and test-plan phases. Research existing coverage before filling this in.

      Keep it concise (1-2 pages). Focus on the "why" not the "how" —
      implementation details belong in design.md.

      This is the foundation — test plans, design, and tasks all build on this.
    requires: []

  - id: test-plan
    generates: test-plan.md
    description: Test scenarios to write — the contract before any code
    template: test-plan.md
    instruction: |
      Create the test plan that defines WHAT tests to write. This is the heart of TDD —
      these scenarios become the failing tests in the RED phase.

      Create one section per domain listed in the intent's Test Domains section.

      Format:
      - Each test group: `## <Domain>` heading
      - Each test: `### Test: <name>` with GIVEN/WHEN/THEN scenarios
      - Include edge cases, error cases, and boundary conditions
      - Tests should be concrete enough to implement directly

      Delta operations for coverage (use ## headers):
      - **ADDED Tests**: New test cases
      - **MODIFIED Tests**: Changed test behavior — MUST include full updated content
      - **REMOVED Tests**: Deprecated tests — MUST include **Reason** and **Migration**

      Example:
           ```
           ## ADDED Tests

           ### Test: User can export data as CSV
           - GIVEN a user with data in their account
           - WHEN the user clicks "Export" and selects CSV format
           - THEN a CSV file downloads containing all user data
           - AND the CSV headers match the data schema

           ### Test: Export fails gracefully with no data
           - GIVEN a user with an empty account
           - WHEN the user clicks "Export"
           - THEN a helpful message is displayed explaining no data exists
           ```

      Every test scenario here will become a real test in the RED phase.
      Make them specific, testable, and complete.
    requires:
      - intent

  - id: design
    generates: design.md
    description: Technical design document with implementation approach
    template: design.md
    instruction: |
      Create the design document that explains HOW to implement the change.

      When to include design.md (create only if any apply):
      - Cross-cutting change (multiple services/modules) or new architectural pattern
      - New external dependency or significant data model changes
      - Security, performance, or migration complexity
      - Ambiguity that benefits from technical decisions before coding

      Sections:
      - **Context**: Background, current state, constraints
      - **Goals / Non-Goals**: What this design achieves and explicitly excludes
      - **Decisions**: Key technical choices with rationale (why X over Y?)
      - **Test Strategy**: How to structure tests — unit vs integration vs e2e, mocking approach, fixtures needed
      - **Risks / Trade-offs**: Known limitations, things that could go wrong

      Focus on architecture and approach, not line-by-line implementation.
      The test strategy section is critical — it informs how the RED phase is approached.
    requires:
      - intent

  - id: tasks
    generates: tasks.md
    description: Red/Green/Refactor implementation checklist
    template: tasks.md
    instruction: |
      Create the task list organized by TDD phases: Red, Green, Refactor.

      **IMPORTANT: Follow the template below exactly.** The apply phases parse
      checkbox format to track progress. Tasks not using `- [ ]` won't be tracked.

      Structure:
      ## 1. RED — Write Failing Tests
      Tasks for writing test files. Each test from the test plan becomes a task.
      Tests MUST fail when first written (no implementation exists yet).

      ## 2. GREEN — Make Tests Pass
      Tasks for writing the minimal implementation to make each test pass.
      Order by dependency — implement foundations before features.

      ## 3. REFACTOR — Clean Up
      Tasks for improving code quality without changing behavior.
      All tests must stay green after each refactor task.

      Example:
           ```
           ## 1. RED — Write Failing Tests

           - [ ] 1.1 Write test for user data export
           - [ ] 1.2 Write test for CSV formatting
           - [ ] 1.3 Write test for empty data edge case

           ## 2. GREEN — Make Tests Pass

           - [ ] 2.1 Implement data export function
           - [ ] 2.2 Implement CSV formatter
           - [ ] 2.3 Handle empty data case

           ## 3. REFACTOR — Clean Up

           - [ ] 3.1 Extract shared export utilities
           - [ ] 3.2 Add JSDoc to public API
           ```

      Reference the test plan for RED tasks, design for GREEN tasks.
      Each task should be verifiable — you know when it's done.
    requires:
      - test-plan
      - design

apply:
  requires: [tasks]
  tracks: tasks.md
  instruction: |
    Work through tasks in TDD order: RED first, then GREEN, then REFACTOR.
    Run tests after each phase to confirm the expected state.
